---
layout:     post
title:      模型压缩与加速论文列表
subtitle:   剪枝·量化·蒸馏等
date:       2020-03-25
author:     DT
header-img: img/post-bg-debug.png
catalog: true
tags:
    - model.acceleration
    - paper.list


---

> 模型压缩与加速论文列表

# quantization

## arxiv2002

* [Automatic Pruning for Quantized Neural Networks](https://arxiv.org/pdf/2002.00523.pdf)
* [BitPruning: Learning Bitlengths for Aggressive and Accurate Quantization](https://arxiv.org/pdf/2002.03090.pdf)
* [Post-Training Piecewise Linear Quantization for Deep Neural Networks](https://arxiv.org/pdf/2002.00104.pdf)
* [SQWA: Stochastic Quantized Weight Averaging for Improving the Generalization Capability of Low-Precision Deep Neural Networks](https://arxiv.org/pdf/2002.00343.pdf)
* [Widening and Squeezing: Towards Accurate and Efficient QNNs](https://arxiv.org/pdf/2002.00555.pdf)
* [Towards Explainable Bit Error Tolerance of Resistive RAM-Based Binarized Neural Networks](https://arxiv.org/pdf/2002.00909.pdf)
* [Switchable Precision Neural Networks](https://arxiv.org/pdf/2002.02815.pdf)

# pruning

## arxiv2002

* [Automatic Pruning for Quantized Neural Networks](https://arxiv.org/pdf/2002.00523.pdf)
* [Proving the Lottery Ticket Hypothesis: Pruning is All You Need](https://arxiv.org/pdf/2002.00585.pdf)
* [Activation Density driven Energy-Efficient Pruning in Training](https://arxiv.org/pdf/2002.02949.pdf)
* [Convolutional Neural Network Pruning Using Filter Attenuation](https://arxiv.org/pdf/2002.03299.pdf)
* [Lookahead: A Far-Sighted Alternative of Magnitude-based Pruning](https://arxiv.org/pdf/2002.04809.pdf)
* [PCNN: Pattern-based Fine-Grained Regular Pruning towards Optimizing CNN Accelerators](https://arxiv.org/pdf/2002.04997.pdf)
* [Layer-wise Pruning and Auto-tuning of Layer-wise Learning Rates in Fine-tuning of Deep Networks](https://arxiv.org/pdf/2002.06048.pdf)
* [Training Efficient Network Architecture and Weights via Direct Sparsity Control](https://arxiv.org/pdf/2002.04301.pdf)

# distillation

## arxiv2002

* [Understanding and Improving Knowledge Distillation](https://arxiv.org/pdf/2002.03532.pdf)
* 

# NAS

## arxiv2002

* 

# others

## arxiv2002

* 【AAAI2020】[DWM: A Decomposable Winograd Method for Convolution Acceleration](https://arxiv.org/pdf/2002.00552.pdf)
* [Accelerating Deep Learning Inference via Freezing](https://arxiv.org/pdf/2002.02645.pdf)





